{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUbvP-jGD74b",
        "outputId": "2db3f3b5-fe80-4384-b694-5f96bc3d08c6"
      },
      "outputs": [],
      "source": [
        "# ===== Basic =====\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import cv2\n",
        "\n",
        "# ===== CLIP (Spotlight) =====\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ===== V2P =====\n",
        "sys.path.append('/home/phamlong/Downloads/v2p/AWorld-RL/V2P/src')\n",
        "from transformers import AutoProcessor\n",
        "from V2P.constants import DEFAULT_POINTER_PAD_TOKEN, DEFAULT_POINTER_END_TOKEN\n",
        "from V2P.modeling import Qwen2VLForConditionalGenerationWithPointer\n",
        "from V2P.modeling_qwen25vl import Qwen2_5_VLForConditionalGenerationWithPointer\n",
        "from V2P.inference import inference, ForceFollowTokensLogitsProcessor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TOP_K_REGIONS = 2\n",
        "REGION_THRESH = 0.28\n",
        "\n",
        "FEATHER_SIGMA = 16        # giống feather_sigma ver5\n",
        "SOFT_MASK_THRESH = 0.25  # ngưỡng để lấy bbox\n",
        "CROP_PAD = 8              # tuỳ em\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading CLIP...\n"
          ]
        }
      ],
      "source": [
        "# ================= 2️⃣ Cấu hình các tham số =================\n",
        "MODEL_TYPE = \"qwen25vl\"  \n",
        "MODEL_NAME_OR_PATH = \"inclusionAI/V2P-7B\"  \n",
        "# MODEL_NAME_OR_PATH = \"microsoft/GUI-Actor-7B-Qwen2.5-VL\"  \n",
        "\n",
        "SAVE_PATH = \"results/\"  # tương đương --save_path\n",
        "DATA_PATH = \"/home/phamlong/Downloads/v2p/ScreenSpot-Pro\"\n",
        "RESIZE_TO_PIXELS = 3200*1800\n",
        "USE_PLACEHOLDER = True\n",
        "TOPK = 3\n",
        "IMAGE_PATCH_SIZE = 14\n",
        "# ===== Spotlight Config =====\n",
        "BASE_CLIP = \"/home/phamlong/Downloads/clip finetune/clip-vit-base-patch32\"\n",
        "CKPT_PATH = \"/home/phamlong/Downloads/clip finetune/clip_finetuned/clip_finetuned_epoch5.pth\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "NUM_GRID = 36\n",
        "TOP_K_REGIONS = 2\n",
        "REGION_THRESH = 0.28\n",
        "CROP_PAD = 8\n",
        "\n",
        "print(\"Loading CLIP...\")\n",
        "clip_model = CLIPModel.from_pretrained(BASE_CLIP)\n",
        "clip_processor = CLIPProcessor.from_pretrained(BASE_CLIP)\n",
        "\n",
        "if os.path.exists(CKPT_PATH):\n",
        "    state = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
        "    if \"model\" in state:\n",
        "        state = state[\"model\"]\n",
        "    clip_model.load_state_dict(state, strict=False)\n",
        "\n",
        "clip_model = clip_model.to(DEVICE).eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "\n",
        "IMAGE_DIR = os.path.join(DATA_PATH, \"images\")\n",
        "DATA_FN = os.path.join(DATA_PATH, \"/home/phamlong/Downloads/v2p/ScreenSpot-Pro/merged_annotations.json\")\n",
        "PRED_PATH = os.path.join(SAVE_PATH, \"screenspot-Pro_all_preds_StandardResize.json\")\n",
        "METRIC_PATH = os.path.join(SAVE_PATH, \"screenspot-Pro_all_preds_StandardResize.txt\")\n",
        "annotation_files = sorted(glob(os.path.join(DATA_FN, \"*.json\")))\n",
        "if not os.path.exists(SAVE_PATH):\n",
        "    os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# ================= 3️⃣ Hàm hỗ trợ =================\n",
        "def normalize_bbox(bbox_x1y1x2y2, img_width, img_height):\n",
        "    x1, y1, x2, y2 = bbox_x1y1x2y2\n",
        "    if (0 <= x1 <= 1) and (0 <= y1 <= 1) and (0 <= x2 <= 1) and (0 <= y2 <= 1):\n",
        "        return bbox_x1y1x2y2\n",
        "    else:\n",
        "        x1 = x1 / img_width\n",
        "        y1 = y1 / img_height\n",
        "        x2 = x2 / img_width\n",
        "        y2 = y2 / img_height\n",
        "        return x1, y1, x2, y2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hybrid_vector_field_grid(image, instruction, num_grid=36, sigma=0.4, batch_size=8):\n",
        "    img = image.convert(\"RGB\")\n",
        "    W, H = img.size\n",
        "    pw, ph = W // num_grid, H // num_grid\n",
        "\n",
        "    patches = []\n",
        "    for gy in range(num_grid):\n",
        "        for gx in range(num_grid):\n",
        "            x1, y1 = gx * pw, gy * ph\n",
        "            x2, y2 = x1 + pw, y1 + ph\n",
        "            patches.append(img.crop((x1, y1, x2, y2)))\n",
        "\n",
        "    # text embedding\n",
        "    text_inputs = clip_processor(text=instruction, return_tensors=\"pt\").to(DEVICE)\n",
        "    text_emb = clip_model.get_text_features(**text_inputs)\n",
        "    text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
        "    text_emb = text_emb.cpu().numpy()\n",
        "\n",
        "    # patch embeddings\n",
        "    feats = []\n",
        "    for i in range(0, len(patches), batch_size):\n",
        "        inputs = clip_processor(images=patches[i:i+batch_size], return_tensors=\"pt\").to(DEVICE)\n",
        "        emb = clip_model.get_image_features(**inputs)\n",
        "        emb = emb / emb.norm(dim=-1, keepdim=True)\n",
        "        feats.append(emb.cpu().numpy())\n",
        "    patch_embs = np.concatenate(feats, axis=0)\n",
        "\n",
        "    cos_sim = cosine_similarity(patch_embs, text_emb).flatten()\n",
        "    gates = 1 / (1 + np.exp(-3.0 * (cos_sim - 0.5)))\n",
        "    gated = patch_embs * gates[:, None]\n",
        "\n",
        "    N = gated.shape[0]\n",
        "    F_field = np.zeros_like(gated)\n",
        "    for i in range(N):\n",
        "        diff = gated - gated[i]\n",
        "        dist = np.linalg.norm(diff, axis=1)\n",
        "        w = np.exp(-dist**2 / sigma**2) * gates\n",
        "        F_field[i] = np.sum(w[:, None] * diff, axis=0)\n",
        "\n",
        "    F_field /= (np.linalg.norm(F_field, axis=1, keepdims=True) + 1e-8)\n",
        "\n",
        "    div = np.zeros(N)\n",
        "    for i in range(N):\n",
        "        idx = np.argsort(np.linalg.norm(F_field - F_field[i], axis=1))[:8]\n",
        "        grad = (F_field[idx] - F_field[i]).mean(axis=0)\n",
        "        div[i] = np.dot(grad, F_field[i])\n",
        "\n",
        "    score = np.log1p(np.maximum(0, -div)) * cos_sim * gates\n",
        "    score = (score - score.min()) / (score.max() - score.min() + 1e-8)\n",
        "\n",
        "    return score.reshape(num_grid, num_grid), (H, W)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def spotlight_bbox(image, instruction):\n",
        "    grid_scores, (H, W) = hybrid_vector_field_grid(image, instruction)\n",
        "\n",
        "    binary = (grid_scores >= REGION_THRESH).astype(np.uint8)\n",
        "    num_labels, labels = cv2.connectedComponents(binary)\n",
        "\n",
        "    regions = []\n",
        "    for l in range(1, num_labels):\n",
        "        mask = (labels == l)\n",
        "        area = mask.sum()\n",
        "        if area < 3:\n",
        "            continue\n",
        "        score = grid_scores[mask].mean() * area\n",
        "        regions.append((score, l))\n",
        "\n",
        "    regions = sorted(regions, reverse=True)[:TOP_K_REGIONS]\n",
        "\n",
        "    if len(regions) == 0:\n",
        "        return None, None\n",
        "\n",
        "    # ---- patch mask ----\n",
        "    patch_mask = np.zeros_like(grid_scores, dtype=np.float32)\n",
        "    for _, l in regions:\n",
        "        patch_mask += (labels == l).astype(np.float32)\n",
        "    patch_mask = np.clip(patch_mask, 0, 1)\n",
        "\n",
        "    # ---- upsample (GIỐNG ver5) ----\n",
        "    t = torch.tensor(patch_mask)[None, None]\n",
        "    mask_full = F.interpolate(\n",
        "        t, size=(H, W), mode=\"bicubic\", align_corners=False\n",
        "    ).squeeze().numpy()\n",
        "\n",
        "    mask_full = (mask_full - mask_full.min()) / (mask_full.max() - mask_full.min() + 1e-8)\n",
        "\n",
        "    # ---- feather ----\n",
        "    mask_blur = cv2.GaussianBlur(mask_full, (0, 0), FEATHER_SIGMA)\n",
        "\n",
        "    # ---- bbox từ mask ----\n",
        "    bin_mask = mask_blur > 0.25\n",
        "    ys, xs = np.where(bin_mask)\n",
        "    if len(xs) == 0:\n",
        "        return None, None\n",
        "\n",
        "    x1, x2 = xs.min(), xs.max()\n",
        "    y1, y2 = ys.min(), ys.max()\n",
        "\n",
        "    bbox = (\n",
        "        max(0, x1 - CROP_PAD),\n",
        "        max(0, y1 - CROP_PAD),\n",
        "        min(W - 1, x2 + CROP_PAD),\n",
        "        min(H - 1, y2 + CROP_PAD),\n",
        "    )\n",
        "\n",
        "    return bbox, mask_blur\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def evaluate(\n",
        "#     model_name_or_path,\n",
        "#     model_type,\n",
        "#     data_fn,\n",
        "#     image_dir,\n",
        "#     use_placeholder,\n",
        "#     topk,\n",
        "#     resize_to_pixels=None\n",
        "# ):\n",
        "#     import os, json, torch\n",
        "#     from PIL import Image, ImageDraw\n",
        "#     from tqdm import tqdm\n",
        "#     from transformers import AutoProcessor\n",
        "\n",
        "#     # ================= LOAD PROCESSOR & TOKENIZER =================\n",
        "#     data_processor = AutoProcessor.from_pretrained(model_name_or_path)\n",
        "#     tokenizer = data_processor.tokenizer\n",
        "\n",
        "#     if model_type == \"qwen2vl\":\n",
        "#         model = Qwen2VLForConditionalGenerationWithPointer.from_pretrained(\n",
        "#             model_name_or_path,\n",
        "#             torch_dtype=torch.bfloat16,\n",
        "#             device_map=\"cuda:0\",\n",
        "#             attn_implementation=\"flash_attention_2\"\n",
        "#         ).eval()\n",
        "\n",
        "#         grounding_system_message = (\n",
        "#             \"You are a GUI agent. You are given a task and a screenshot of the screen. \"\n",
        "#             \"You need to locate the UI element corresponding to the instruction.\"\n",
        "#         )\n",
        "\n",
        "#     elif model_type == \"qwen25vl\":\n",
        "#         model = Qwen2_5_VLForConditionalGenerationWithPointer.from_pretrained(\n",
        "#             model_name_or_path,\n",
        "#             torch_dtype=torch.bfloat16,\n",
        "#             device_map=\"cuda:0\",\n",
        "#             attn_implementation=\"flash_attention_2\"\n",
        "#         ).eval()\n",
        "\n",
        "#         grounding_system_message = (\n",
        "#             \"You are a GUI agent. Given a screenshot of the current GUI and a human instruction, \"\n",
        "#             \"your task is to locate the screen element that corresponds to the instruction.\"\n",
        "#         )\n",
        "#     else:\n",
        "#         raise ValueError(\"Invalid model type\")\n",
        "\n",
        "#     logits_processor_pointer = ForceFollowTokensLogitsProcessor(\n",
        "#         token_a_id=tokenizer.encode(DEFAULT_POINTER_PAD_TOKEN)[0],\n",
        "#         forced_sequence=[tokenizer.encode(DEFAULT_POINTER_END_TOKEN)[0]]\n",
        "#     )\n",
        "\n",
        "#     # ================= LOAD DATA =================\n",
        "#     with open(data_fn, \"r\") as f:\n",
        "#         data = json.load(f)\n",
        "\n",
        "#     results = []\n",
        "#     os.makedirs(\"vis_debug\", exist_ok=True)\n",
        "\n",
        "#     # ================= MAIN LOOP =================\n",
        "#     for example in tqdm(data):\n",
        "\n",
        "#         # ---------- metadata ----------\n",
        "#         ele = {\n",
        "#             \"file_name\": example[\"img_filename\"],\n",
        "#             \"ui_type\": example[\"ui_type\"],\n",
        "#             \"group\": example[\"group\"],\n",
        "#             \"platform\": example[\"platform\"],\n",
        "#             \"application\": example[\"application\"],\n",
        "#             \"id\": example[\"id\"],\n",
        "#             \"instruction\": example[\"instruction\"],\n",
        "#             \"img_size\": example[\"img_size\"],\n",
        "#             \"bbox_x1y1x2y2\": normalize_bbox(\n",
        "#                 example[\"bbox\"],\n",
        "#                 example[\"img_size\"][0],\n",
        "#                 example[\"img_size\"][1]\n",
        "#             ),\n",
        "#             \"hit_top1\": 0,\n",
        "#         }\n",
        "\n",
        "#         # ---------- load image ----------\n",
        "#         image = Image.open(\n",
        "#             os.path.join(image_dir, example[\"img_filename\"])\n",
        "#         ).convert(\"RGB\")\n",
        "#         W, H = image.size\n",
        "\n",
        "#         # ================= STAGE 0: SPOTLIGHT ROI =================\n",
        "#         # roi_bbox = spotlight_bbox(image, example[\"instruction\"])\n",
        "#         roi_bbox, mask_blur = spotlight_bbox(image, example[\"instruction\"])\n",
        "\n",
        "#         if roi_bbox is None:\n",
        "#             rx1, ry1, rx2, ry2 = 0, 0, W, H\n",
        "#         else:\n",
        "#             rx1, ry1, rx2, ry2 = roi_bbox\n",
        "\n",
        "#         roi = image.crop((rx1, ry1, rx2, ry2))\n",
        "#         roi_w, roi_h = roi.size\n",
        "\n",
        "#         # ================= STAGE 1: TOP-3 COARSE GUESS =================\n",
        "#         conv1 = [\n",
        "#             {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": grounding_system_message}]},\n",
        "#             {\"role\": \"user\", \"content\": [\n",
        "#                 {\"type\": \"image\", \"image\": roi},\n",
        "#                 {\"type\": \"text\", \"text\": example[\"instruction\"]}\n",
        "#             ]}\n",
        "#         ]\n",
        "\n",
        "#         pred1 = inference(\n",
        "#             conv1,\n",
        "#             model,\n",
        "#             tokenizer,\n",
        "#             data_processor,\n",
        "#             logits_processor=logits_processor_pointer,\n",
        "#             use_placeholder=use_placeholder,\n",
        "#             topk=3\n",
        "#         )\n",
        "\n",
        "#         top3_roi = pred1[\"topk_points\"][:3]\n",
        "\n",
        "#         # remap to global px\n",
        "#         pts = []\n",
        "#         for px, py in top3_roi:\n",
        "#             gx = px * roi_w + rx1\n",
        "#             gy = py * roi_h + ry1\n",
        "#             pts.append((gx, gy))\n",
        "\n",
        "#         # ================= STAGE 2: TIGHT CROP + ZOOM =================\n",
        "#         xs, ys = zip(*pts)\n",
        "#         xmin, xmax = min(xs), max(xs)\n",
        "#         ymin, ymax = min(ys), max(ys)\n",
        "\n",
        "#         margin = 0.25\n",
        "#         bw, bh = xmax - xmin, ymax - ymin\n",
        "#         xmin = max(0, int(xmin - bw * margin))\n",
        "#         ymin = max(0, int(ymin - bh * margin))\n",
        "#         xmax = min(W, int(xmax + bw * margin))\n",
        "#         ymax = min(H, int(ymax + bh * margin))\n",
        "\n",
        "#         crop1 = image.crop((xmin, ymin, xmax, ymax))\n",
        "#         zoom1 = crop1.resize((W, H), Image.BILINEAR)\n",
        "\n",
        "#         conv2 = [\n",
        "#             {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": grounding_system_message}]},\n",
        "#             {\"role\": \"user\", \"content\": [\n",
        "#                 {\"type\": \"image\", \"image\": zoom1},\n",
        "#                 {\"type\": \"text\", \"text\": example[\"instruction\"]}\n",
        "#             ]}\n",
        "#         ]\n",
        "\n",
        "#         pred2 = inference(\n",
        "#             conv2,\n",
        "#             model,\n",
        "#             tokenizer,\n",
        "#             data_processor,\n",
        "#             logits_processor=logits_processor_pointer,\n",
        "#             use_placeholder=use_placeholder,\n",
        "#             topk=1\n",
        "#         )\n",
        "\n",
        "#         zx1, zy1 = pred2[\"topk_points\"][0]\n",
        "#         px1 = (xmin + zx1 * (xmax - xmin)) / W\n",
        "#         py1 = (ymin + zy1 * (ymax - ymin)) / H\n",
        "\n",
        "#         # ================= STAGE 3: FINAL CONFIRM ZOOM =================\n",
        "#         cx, cy = int(px1 * W), int(py1 * H)\n",
        "#         box = int(0.25 * min(W, H))\n",
        "\n",
        "#         x1 = max(0, cx - box)\n",
        "#         y1 = max(0, cy - box)\n",
        "#         x2 = min(W, cx + box)\n",
        "#         y2 = min(H, cy + box)\n",
        "\n",
        "#         crop2 = image.crop((x1, y1, x2, y2))\n",
        "#         zoom2 = crop2.resize((W, H), Image.BILINEAR)\n",
        "\n",
        "#         conv3 = [\n",
        "#             {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": grounding_system_message}]},\n",
        "#             {\"role\": \"user\", \"content\": [\n",
        "#                 {\"type\": \"image\", \"image\": zoom2},\n",
        "#                 {\"type\": \"text\", \"text\": example[\"instruction\"]}\n",
        "#             ]}\n",
        "#         ]\n",
        "\n",
        "#         pred3 = inference(\n",
        "#             conv3,\n",
        "#             model,\n",
        "#             tokenizer,\n",
        "#             data_processor,\n",
        "#             logits_processor=logits_processor_pointer,\n",
        "#             use_placeholder=use_placeholder,\n",
        "#             topk=1\n",
        "#         )\n",
        "\n",
        "#         zx2, zy2 = pred3[\"topk_points\"][0]\n",
        "#         final_px = (x1 + zx2 * (x2 - x1)) / W\n",
        "#         final_py = (y1 + zy2 * (y2 - y1)) / H\n",
        "\n",
        "#         # ================= EVALUATION =================\n",
        "#         gx1, gy1, gx2, gy2 = ele[\"bbox_x1y1x2y2\"]\n",
        "#         if gx1 <= final_px <= gx2 and gy1 <= final_py <= gy2:\n",
        "#             ele[\"hit_top1\"] = 1\n",
        "\n",
        "#         #================= VIS DEBUG =================\n",
        "#         # vis = image.copy()\n",
        "#         # draw = ImageDraw.Draw(vis)\n",
        "\n",
        "#         # # 1️⃣ GT bbox (green)\n",
        "#         # draw.rectangle(\n",
        "#         #     [\n",
        "#         #         int(gx1 * W),\n",
        "#         #         int(gy1 * H),\n",
        "#         #         int(gx2 * W),\n",
        "#         #         int(gy2 * H)\n",
        "#         #     ],\n",
        "#         #     outline=\"lime\",\n",
        "#         #     width=3\n",
        "#         # )\n",
        "\n",
        "#         # # 2️⃣ Spotlight crop bbox (yellow)\n",
        "#         # draw.rectangle(\n",
        "#         #     [rx1, ry1, rx2, ry2],\n",
        "#         #     outline=\"yellow\",\n",
        "#         #     width=2\n",
        "#         # )\n",
        "\n",
        "#         # # 3️⃣ Final confirmed Top-1 point (red)\n",
        "#         # fx, fy = int(final_px * W), int(final_py * H)\n",
        "#         # r = 6\n",
        "#         # draw.ellipse(\n",
        "#         #     [fx - r, fy - r, fx + r, fy + r],\n",
        "#         #     fill=\"red\"\n",
        "#         # )\n",
        "\n",
        "#         # vis.save(f\"vis_debug/{example['id']}_final.png\")\n",
        "#         # ================= VIS WITH SOFT SPOTLIGHT MASK =================\n",
        "#         img_np = np.array(image).astype(np.float32) / 255.0\n",
        "\n",
        "#         # ===============================\n",
        "#         # Apply soft spotlight (VIS ONLY)\n",
        "#         # ===============================\n",
        "#         if mask_blur is not None:\n",
        "#             darken_factor = 0.07\n",
        "\n",
        "#             # ensure mask shape [H, W]\n",
        "#             if mask_blur.ndim == 3:\n",
        "#                 mask_blur = mask_blur.squeeze(-1)\n",
        "\n",
        "#             mask3 = np.stack([mask_blur] * 3, axis=-1)\n",
        "\n",
        "#             dark_np = img_np * darken_factor\n",
        "#             vis_np = dark_np * (1 - mask3) + img_np * mask3\n",
        "#             vis_np = np.clip(vis_np, 0, 1)\n",
        "#         else:\n",
        "#             # fallback: no spotlight, use original image\n",
        "#             vis_np = img_np\n",
        "\n",
        "#         vis = Image.fromarray((vis_np * 255).astype(np.uint8))\n",
        "#         draw = ImageDraw.Draw(vis)\n",
        "\n",
        "\n",
        "#         # 1️⃣ GT bbox (green)\n",
        "#         draw.rectangle(\n",
        "#             [\n",
        "#                 int(gx1 * W),\n",
        "#                 int(gy1 * H),\n",
        "#                 int(gx2 * W),\n",
        "#                 int(gy2 * H)\n",
        "#             ],\n",
        "#             outline=\"lime\",\n",
        "#             width=3\n",
        "#         )\n",
        "\n",
        "#         # 2️⃣ Spotlight crop bbox (yellow)\n",
        "#         draw.rectangle(\n",
        "#             [rx1, ry1, rx2, ry2],\n",
        "#             outline=\"yellow\",\n",
        "#             width=2\n",
        "#         )\n",
        "\n",
        "#         # 3️⃣ Final confirmed Top-1 point (red)\n",
        "#         fx, fy = int(final_px * W), int(final_py * H)\n",
        "#         r = 6\n",
        "#         draw.ellipse(\n",
        "#             [fx - r, fy - r, fx + r, fy + r],\n",
        "#             fill=\"red\"\n",
        "#         )\n",
        "\n",
        "#         os.makedirs(\"vis_debug\", exist_ok=True)\n",
        "#         vis.save(f\"vis_debug/{example['id']}_final.png\")\n",
        "\n",
        "\n",
        "\n",
        "#         results.append(ele)\n",
        "\n",
        "#     return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#no add stage#\n",
        "def evaluate(\n",
        "    model_name_or_path,\n",
        "    model_type,\n",
        "    data_fn,\n",
        "    image_dir,\n",
        "    use_placeholder,\n",
        "    topk,\n",
        "    resize_to_pixels=None\n",
        "):\n",
        "    import os, json, torch\n",
        "    import numpy as np\n",
        "    from PIL import Image, ImageDraw\n",
        "    from tqdm import tqdm\n",
        "    from transformers import AutoProcessor\n",
        "\n",
        "    # ================= LOAD PROCESSOR & TOKENIZER =================\n",
        "    data_processor = AutoProcessor.from_pretrained(model_name_or_path)\n",
        "    tokenizer = data_processor.tokenizer\n",
        "\n",
        "    if model_type == \"qwen2vl\":\n",
        "        model = Qwen2VLForConditionalGenerationWithPointer.from_pretrained(\n",
        "            model_name_or_path,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"cuda:0\",\n",
        "            attn_implementation=\"flash_attention_2\"\n",
        "        ).eval()\n",
        "\n",
        "        grounding_system_message = (\n",
        "            \"You are a GUI agent. You are given a task and a screenshot of the screen. \"\n",
        "            \"You need to locate the UI element corresponding to the instruction.\"\n",
        "        )\n",
        "\n",
        "    elif model_type == \"qwen25vl\":\n",
        "        model = Qwen2_5_VLForConditionalGenerationWithPointer.from_pretrained(\n",
        "            model_name_or_path,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"cuda:0\",\n",
        "            attn_implementation=\"flash_attention_2\"\n",
        "        ).eval()\n",
        "\n",
        "        grounding_system_message = (\n",
        "            \"You are a GUI agent. Given a screenshot of the current GUI and a human instruction, \"\n",
        "            \"your task is to locate the screen element that corresponds to the instruction.\"\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type\")\n",
        "\n",
        "    logits_processor_pointer = ForceFollowTokensLogitsProcessor(\n",
        "        token_a_id=tokenizer.encode(DEFAULT_POINTER_PAD_TOKEN)[0],\n",
        "        forced_sequence=[tokenizer.encode(DEFAULT_POINTER_END_TOKEN)[0]]\n",
        "    )\n",
        "\n",
        "    # ================= LOAD DATA =================\n",
        "    with open(data_fn, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    results = []\n",
        "    os.makedirs(\"vis_debug\", exist_ok=True)\n",
        "\n",
        "    for example in tqdm(data):\n",
        "\n",
        "        # ---------- instruction selection ----------\n",
        "        instruction_text = example[\"instruction\"]\n",
        "        fname = example.get(\"img_filename\", \"\").lower()\n",
        "\n",
        "        if fname.startswith(\"autocad_windows\") or fname.startswith(\"solidworks_windows\"):\n",
        "            if \"instruction_cn\" in example and example[\"instruction_cn\"]:\n",
        "                instruction_text = example[\"instruction_cn\"]\n",
        "\n",
        "        # ---------- metadata ----------\n",
        "        ele = {\n",
        "            \"file_name\": example[\"img_filename\"],\n",
        "            \"ui_type\": example[\"ui_type\"],\n",
        "            \"group\": example[\"group\"],\n",
        "            \"platform\": example[\"platform\"],\n",
        "            \"application\": example[\"application\"],\n",
        "            \"id\": example[\"id\"],\n",
        "            \"instruction\": instruction_text,\n",
        "            \"img_size\": example[\"img_size\"],\n",
        "            \"bbox_x1y1x2y2\": normalize_bbox(\n",
        "                example[\"bbox\"],\n",
        "                example[\"img_size\"][0],\n",
        "                example[\"img_size\"][1]\n",
        "            ),\n",
        "            \"hit_top1\": 0,\n",
        "        }\n",
        "\n",
        "        # ---------- load image ----------\n",
        "        image = Image.open(\n",
        "            os.path.join(image_dir, example[\"img_filename\"])\n",
        "        ).convert(\"RGB\")\n",
        "        W, H = image.size\n",
        "\n",
        "        roi_bbox, mask_blur = spotlight_bbox(image, instruction_text)\n",
        "\n",
        "        if roi_bbox is None:\n",
        "            rx1, ry1, rx2, ry2 = 0, 0, W, H\n",
        "        else:\n",
        "            rx1, ry1, rx2, ry2 = roi_bbox\n",
        "\n",
        "        roi = image.crop((rx1, ry1, rx2, ry2))\n",
        "        roi_w, roi_h = roi.size\n",
        "\n",
        "        conv1 = [\n",
        "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": grounding_system_message}]},\n",
        "            {\"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"image\", \"image\": roi},\n",
        "                {\"type\": \"text\", \"text\": instruction_text}\n",
        "            ]}\n",
        "        ]\n",
        "\n",
        "        pred1 = inference(\n",
        "            conv1,\n",
        "            model,\n",
        "            tokenizer,\n",
        "            data_processor,\n",
        "            logits_processor=logits_processor_pointer,\n",
        "            use_placeholder=use_placeholder,\n",
        "            topk=3\n",
        "        )\n",
        "\n",
        "        pts = []\n",
        "        for px, py in pred1[\"topk_points\"][:3]:\n",
        "            gx = px * roi_w + rx1\n",
        "            gy = py * roi_h + ry1\n",
        "            pts.append((gx, gy))\n",
        "\n",
        "        xs, ys = zip(*pts)\n",
        "        xmin, xmax = min(xs), max(xs)\n",
        "        ymin, ymax = min(ys), max(ys)\n",
        "\n",
        "        margin = 0.25\n",
        "        bw, bh = xmax - xmin, ymax - ymin\n",
        "        xmin = max(0, int(xmin - bw * margin))\n",
        "        ymin = max(0, int(ymin - bh * margin))\n",
        "        xmax = min(W, int(xmax + bw * margin))\n",
        "        ymax = min(H, int(ymax + bh * margin))\n",
        "\n",
        "        crop1 = image.crop((xmin, ymin, xmax, ymax))\n",
        "        zoom1 = crop1.resize((W, H), Image.BILINEAR)\n",
        "\n",
        "        conv2 = [\n",
        "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": grounding_system_message}]},\n",
        "            {\"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"image\", \"image\": zoom1},\n",
        "                {\"type\": \"text\", \"text\": instruction_text}\n",
        "            ]}\n",
        "        ]\n",
        "\n",
        "        pred2 = inference(\n",
        "            conv2,\n",
        "            model,\n",
        "            tokenizer,\n",
        "            data_processor,\n",
        "            logits_processor=logits_processor_pointer,\n",
        "            use_placeholder=use_placeholder,\n",
        "            topk=3\n",
        "        )\n",
        "\n",
        "        zx, zy = pred2[\"topk_points\"][0]\n",
        "        px_mid = (xmin + zx * (xmax - xmin)) / W\n",
        "        py_mid = (ymin + zy * (ymax - ymin)) / H\n",
        "\n",
        "        cx, cy = int(px_mid * W), int(py_mid * H)\n",
        "        box = int(0.25 * min(W, H))\n",
        "\n",
        "        x1 = max(0, cx - box)\n",
        "        y1 = max(0, cy - box)\n",
        "        x2 = min(W, cx + box)\n",
        "        y2 = min(H, cy + box)\n",
        "\n",
        "        crop2 = image.crop((x1, y1, x2, y2))\n",
        "        zoom2 = crop2.resize((W, H), Image.BILINEAR)\n",
        "\n",
        "        conv3 = [\n",
        "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": grounding_system_message}]},\n",
        "            {\"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"image\", \"image\": zoom2},\n",
        "                {\"type\": \"text\", \"text\": instruction_text}\n",
        "            ]}\n",
        "        ]\n",
        "\n",
        "        pred3 = inference(\n",
        "            conv3,\n",
        "            model,\n",
        "            tokenizer,\n",
        "            data_processor,\n",
        "            logits_processor=logits_processor_pointer,\n",
        "            use_placeholder=use_placeholder,\n",
        "            topk=1\n",
        "        )\n",
        "\n",
        "        zx2, zy2 = pred3[\"topk_points\"][0]\n",
        "        final_px = (x1 + zx2 * (x2 - x1)) / W\n",
        "        final_py = (y1 + zy2 * (y2 - y1)) / H\n",
        "\n",
        "        gx1, gy1, gx2, gy2 = ele[\"bbox_x1y1x2y2\"]\n",
        "        if gx1 <= final_px <= gx2 or gy1 <= final_py <= gy2:\n",
        "            ele[\"hit_top1\"] = 1\n",
        "\n",
        "        # ================= VIS DEBUG =================\n",
        "        img_np = np.array(image).astype(np.float32) / 255.0\n",
        "\n",
        "        if mask_blur is not None:\n",
        "            darken_factor = 0.07\n",
        "            if mask_blur.ndim == 3:\n",
        "                mask_blur = mask_blur.squeeze(-1)\n",
        "            mask3 = np.stack([mask_blur] * 3, axis=-1)\n",
        "            dark_np = img_np * darken_factor\n",
        "            vis_np = dark_np * (1 - mask3) + img_np * mask3\n",
        "            vis_np = np.clip(vis_np, 0, 1)\n",
        "        else:\n",
        "            vis_np = img_np\n",
        "\n",
        "        vis = Image.fromarray((vis_np * 255).astype(np.uint8))\n",
        "        draw = ImageDraw.Draw(vis)\n",
        "\n",
        "        draw.rectangle(\n",
        "            [int(gx1 * W), int(gy1 * H), int(gx2 * W), int(gy2 * H)],\n",
        "            outline=\"lime\", width=3\n",
        "        )\n",
        "        draw.rectangle([rx1, ry1, rx2, ry2], outline=\"yellow\", width=2)\n",
        "\n",
        "        fx, fy = int(final_px * W), int(final_py * H)\n",
        "        r = 6\n",
        "        draw.ellipse([fx - r, fy - r, fx + r, fy + r], fill=\"red\")\n",
        "\n",
        "        vis.save(f\"vis_debug/{example['id']}_final.png\")\n",
        "\n",
        "        results.append(ele)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # add stage#\n",
        "# def evaluate(\n",
        "#     model_name_or_path,\n",
        "#     model_type,\n",
        "#     data_fn,\n",
        "#     image_dir,\n",
        "#     use_placeholder,\n",
        "#     topk,\n",
        "#     resize_to_pixels=None\n",
        "# ):\n",
        "#     import os, json, torch\n",
        "#     import numpy as np\n",
        "#     from PIL import Image, ImageDraw\n",
        "#     from tqdm import tqdm\n",
        "#     from transformers import AutoProcessor\n",
        "\n",
        "#     # ================= LOAD PROCESSOR & TOKENIZER =================\n",
        "#     data_processor = AutoProcessor.from_pretrained(model_name_or_path)\n",
        "#     tokenizer = data_processor.tokenizer\n",
        "\n",
        "#     if model_type == \"qwen2vl\":\n",
        "#         model = Qwen2VLForConditionalGenerationWithPointer.from_pretrained(\n",
        "#             model_name_or_path,\n",
        "#             torch_dtype=torch.bfloat16,\n",
        "#             device_map=\"cuda:0\",\n",
        "#             attn_implementation=\"flash_attention_2\"\n",
        "#         ).eval()\n",
        "\n",
        "#         grounding_system_message = (\n",
        "#             \"You are a GUI agent. You are given a task and a screenshot of the screen. \"\n",
        "#             \"You need to locate the UI element corresponding to the instruction.\"\n",
        "#         )\n",
        "\n",
        "#     elif model_type == \"qwen25vl\":\n",
        "#         model = Qwen2_5_VLForConditionalGenerationWithPointer.from_pretrained(\n",
        "#             model_name_or_path,\n",
        "#             torch_dtype=torch.bfloat16,\n",
        "#             device_map=\"cuda:0\",\n",
        "#             attn_implementation=\"flash_attention_2\"\n",
        "#         ).eval()\n",
        "\n",
        "#         grounding_system_message = (\n",
        "#             \"You are a GUI agent. Given a screenshot of the current GUI and a human instruction, \"\n",
        "#             \"your task is to locate the screen element that corresponds to the instruction.\"\n",
        "#         )\n",
        "#     else:\n",
        "#         raise ValueError(\"Invalid model type\")\n",
        "\n",
        "#     logits_processor_pointer = ForceFollowTokensLogitsProcessor(\n",
        "#         token_a_id=tokenizer.encode(DEFAULT_POINTER_PAD_TOKEN)[0],\n",
        "#         forced_sequence=[tokenizer.encode(DEFAULT_POINTER_END_TOKEN)[0]]\n",
        "#     )\n",
        "\n",
        "#     # ================= LOAD DATA =================\n",
        "#     with open(data_fn, \"r\") as f:\n",
        "#         data = json.load(f)\n",
        "\n",
        "#     results = []\n",
        "#     os.makedirs(\"vis_debug\", exist_ok=True)\n",
        "\n",
        "#     # ================= MAIN LOOP =================\n",
        "#     for example in tqdm(data):\n",
        "\n",
        "#         # ---------- instruction selection (KEY CHANGE) ----------\n",
        "#         instruction_text = example[\"instruction\"]\n",
        "#         fname = example.get(\"img_filename\", \"\").lower()\n",
        "\n",
        "#         if fname.startswith(\"autocad_windows\") or fname.startswith(\"solidworks_windows\"):\n",
        "#             if \"instruction_cn\" in example and example[\"instruction_cn\"]:\n",
        "#                 instruction_text = example[\"instruction_cn\"]\n",
        "\n",
        "#         # ---------- metadata ----------\n",
        "#         ele = {\n",
        "#             \"file_name\": example[\"img_filename\"],\n",
        "#             \"ui_type\": example[\"ui_type\"],\n",
        "#             \"group\": example[\"group\"],\n",
        "#             \"platform\": example[\"platform\"],\n",
        "#             \"application\": example[\"application\"],\n",
        "#             \"id\": example[\"id\"],\n",
        "#             \"instruction\": instruction_text,\n",
        "#             \"img_size\": example[\"img_size\"],\n",
        "#             \"bbox_x1y1x2y2\": normalize_bbox(\n",
        "#                 example[\"bbox\"],\n",
        "#                 example[\"img_size\"][0],\n",
        "#                 example[\"img_size\"][1]\n",
        "#             ),\n",
        "#             \"hit_top1\": 0,\n",
        "#         }\n",
        "\n",
        "#         # ---------- load image ----------\n",
        "#         image = Image.open(\n",
        "#             os.path.join(image_dir, example[\"img_filename\"])\n",
        "#         ).convert(\"RGB\")\n",
        "#         W, H = image.size\n",
        "\n",
        "#         # ================= STAGE 0: SPOTLIGHT ROI =================\n",
        "#         roi_bbox, mask_blur = spotlight_bbox(image, instruction_text)\n",
        "\n",
        "#         if roi_bbox is None:\n",
        "#             rx1, ry1, rx2, ry2 = 0, 0, W, H\n",
        "#         else:\n",
        "#             rx1, ry1, rx2, ry2 = roi_bbox\n",
        "\n",
        "#         roi = image.crop((rx1, ry1, rx2, ry2))\n",
        "#         roi_w, roi_h = roi.size\n",
        "\n",
        "#         # ================= STAGE 1: COARSE TOP-3 =================\n",
        "#         conv1 = [\n",
        "#             {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": grounding_system_message}]},\n",
        "#             {\"role\": \"user\", \"content\": [\n",
        "#                 {\"type\": \"image\", \"image\": roi},\n",
        "#                 {\"type\": \"text\", \"text\": instruction_text}\n",
        "#             ]}\n",
        "#         ]\n",
        "\n",
        "#         pred1 = inference(\n",
        "#             conv1,\n",
        "#             model,\n",
        "#             tokenizer,\n",
        "#             data_processor,\n",
        "#             logits_processor=logits_processor_pointer,\n",
        "#             use_placeholder=use_placeholder,\n",
        "#             topk=3\n",
        "#         )\n",
        "\n",
        "#         pts = []\n",
        "#         for px, py in pred1[\"topk_points\"][:3]:\n",
        "#             gx = px * roi_w + rx1\n",
        "#             gy = py * roi_h + ry1\n",
        "#             pts.append((gx, gy))\n",
        "\n",
        "#         # ================= STAGE 2: ZOOM-REFINE-1 =================\n",
        "#         xs, ys = zip(*pts)\n",
        "#         xmin, xmax = min(xs), max(xs)\n",
        "#         ymin, ymax = min(ys), max(ys)\n",
        "\n",
        "#         margin = 0.25\n",
        "#         bw, bh = xmax - xmin, ymax - ymin\n",
        "#         xmin = max(0, int(xmin - bw * margin))\n",
        "#         ymin = max(0, int(ymin - bh * margin))\n",
        "#         xmax = min(W, int(xmax + bw * margin))\n",
        "#         ymax = min(H, int(ymax + bh * margin))\n",
        "\n",
        "#         crop1 = image.crop((xmin, ymin, xmax, ymax))\n",
        "#         zoom1 = crop1.resize((W, H), Image.BILINEAR)\n",
        "\n",
        "#         conv2 = [\n",
        "#             {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": grounding_system_message}]},\n",
        "#             {\"role\": \"user\", \"content\": [\n",
        "#                 {\"type\": \"image\", \"image\": zoom1},\n",
        "#                 {\"type\": \"text\", \"text\": instruction_text}\n",
        "#             ]}\n",
        "#         ]\n",
        "\n",
        "#         pred2 = inference(\n",
        "#             conv2,\n",
        "#             model,\n",
        "#             tokenizer,\n",
        "#             data_processor,\n",
        "#             logits_processor=logits_processor_pointer,\n",
        "#             use_placeholder=use_placeholder,\n",
        "#             topk=3\n",
        "#         )\n",
        "\n",
        "#         # ================= STAGE 3: ZOOM-REFINE-2 =================\n",
        "#         pts2 = []\n",
        "#         for zx, zy in pred2[\"topk_points\"]:\n",
        "#             gx = xmin + zx * (xmax - xmin)\n",
        "#             gy = ymin + zy * (ymax - ymin)\n",
        "#             pts2.append((gx, gy))\n",
        "\n",
        "#         xs, ys = zip(*pts2)\n",
        "#         xmin2, xmax2 = min(xs), max(xs)\n",
        "#         ymin2, ymax2 = min(ys), max(ys)\n",
        "\n",
        "#         margin = 0.15\n",
        "#         bw, bh = xmax2 - xmin2, ymax2 - ymin2\n",
        "#         xmin2 = max(0, int(xmin2 - bw * margin))\n",
        "#         ymin2 = max(0, int(ymin2 - bh * margin))\n",
        "#         xmax2 = min(W, int(xmax2 + bw * margin))\n",
        "#         ymax2 = min(H, int(ymax2 + bh * margin))\n",
        "\n",
        "#         crop_mid = image.crop((xmin2, ymin2, xmax2, ymax2))\n",
        "#         zoom_mid = crop_mid.resize((W, H), Image.BILINEAR)\n",
        "\n",
        "#         conv_mid = [\n",
        "#             {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": grounding_system_message}]},\n",
        "#             {\"role\": \"user\", \"content\": [\n",
        "#                 {\"type\": \"image\", \"image\": zoom_mid},\n",
        "#                 {\"type\": \"text\", \"text\": instruction_text}\n",
        "#             ]}\n",
        "#         ]\n",
        "\n",
        "#         pred_mid = inference(\n",
        "#             conv_mid,\n",
        "#             model,\n",
        "#             tokenizer,\n",
        "#             data_processor,\n",
        "#             logits_processor=logits_processor_pointer,\n",
        "#             use_placeholder=use_placeholder,\n",
        "#             topk=1\n",
        "#         )\n",
        "\n",
        "#         zx_mid, zy_mid = pred_mid[\"topk_points\"][0]\n",
        "#         px_mid = (xmin2 + zx_mid * (xmax2 - xmin2)) / W\n",
        "#         py_mid = (ymin2 + zy_mid * (ymax2 - ymin2)) / H\n",
        "\n",
        "#         # ================= STAGE 4: FINAL CONFIRM =================\n",
        "#         cx, cy = int(px_mid * W), int(py_mid * H)\n",
        "#         box = int(0.25 * min(W, H))\n",
        "\n",
        "#         x1 = max(0, cx - box)\n",
        "#         y1 = max(0, cy - box)\n",
        "#         x2 = min(W, cx + box)\n",
        "#         y2 = min(H, cy + box)\n",
        "\n",
        "#         crop2 = image.crop((x1, y1, x2, y2))\n",
        "#         zoom2 = crop2.resize((W, H), Image.BILINEAR)\n",
        "\n",
        "#         conv3 = [\n",
        "#             {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": grounding_system_message}]},\n",
        "#             {\"role\": \"user\", \"content\": [\n",
        "#                 {\"type\": \"image\", \"image\": zoom2},\n",
        "#                 {\"type\": \"text\", \"text\": instruction_text}\n",
        "#             ]}\n",
        "#         ]\n",
        "\n",
        "#         pred3 = inference(\n",
        "#             conv3,\n",
        "#             model,\n",
        "#             tokenizer,\n",
        "#             data_processor,\n",
        "#             logits_processor=logits_processor_pointer,\n",
        "#             use_placeholder=use_placeholder,\n",
        "#             topk=1\n",
        "#         )\n",
        "\n",
        "#         zx2, zy2 = pred3[\"topk_points\"][0]\n",
        "#         final_px = (x1 + zx2 * (x2 - x1)) / W\n",
        "#         final_py = (y1 + zy2 * (y2 - y1)) / H\n",
        "\n",
        "#         # ================= EVALUATION =================\n",
        "#         gx1, gy1, gx2, gy2 = ele[\"bbox_x1y1x2y2\"]\n",
        "#         if gx1 <= final_px <= gx2 or gy1 <= final_py <= gy2:\n",
        "#             ele[\"hit_top1\"] = 1\n",
        "\n",
        "#         # ================= VIS DEBUG =================\n",
        "#         img_np = np.array(image).astype(np.float32) / 255.0\n",
        "\n",
        "#         if mask_blur is not None:\n",
        "#             darken_factor = 0.07\n",
        "#             if mask_blur.ndim == 3:\n",
        "#                 mask_blur = mask_blur.squeeze(-1)\n",
        "#             mask3 = np.stack([mask_blur] * 3, axis=-1)\n",
        "#             dark_np = img_np * darken_factor\n",
        "#             vis_np = dark_np * (1 - mask3) + img_np * mask3\n",
        "#             vis_np = np.clip(vis_np, 0, 1)\n",
        "#         else:\n",
        "#             vis_np = img_np\n",
        "\n",
        "#         vis = Image.fromarray((vis_np * 255).astype(np.uint8))\n",
        "#         draw = ImageDraw.Draw(vis)\n",
        "\n",
        "#         draw.rectangle(\n",
        "#             [int(gx1 * W), int(gy1 * H), int(gx2 * W), int(gy2 * H)],\n",
        "#             outline=\"lime\", width=3\n",
        "#         )\n",
        "#         draw.rectangle([rx1, ry1, rx2, ry2], outline=\"yellow\", width=2)\n",
        "\n",
        "#         fx, fy = int(final_px * W), int(final_py * H)\n",
        "#         r = 6\n",
        "#         draw.ellipse([fx - r, fy - r, fx + r, fy + r], fill=\"red\")\n",
        "\n",
        "#         vis.save(f\"vis_debug/{example['id']}_final.png\")\n",
        "\n",
        "#         results.append(ele)\n",
        "\n",
        "#     return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================= 5️⃣ Hàm get_metric =================\n",
        "def get_metric(list_of_examples):\n",
        "    metrics = [\"hit_top1\", \"overlap_top1\", \"hit_topk\", \"overlap_topk\"]\n",
        "    groups=[\"Dev\", \"Creative\", \"CAD\", \"Scientific\", \"Office\", \"OS\"]\n",
        "    ui_types=[\"text\", \"icon\"]\n",
        "\n",
        "    def compute_mean(examples, key):\n",
        "        if not examples: return None\n",
        "        return sum(example.get(key, 0) for example in examples)/len(examples)\n",
        "\n",
        "    results = {metric:{} for metric in metrics}\n",
        "\n",
        "    for group in groups:\n",
        "        group_examples = [ex for ex in list_of_examples if ex.get(\"group\")==group]\n",
        "        for ui in ui_types:\n",
        "            group_ui_examples = [ex for ex in group_examples if ex.get(\"ui_type\")==ui]\n",
        "            col_name=f\"{group}-{ui}\"\n",
        "            for metric in metrics:\n",
        "                results[metric][col_name]=compute_mean(group_ui_examples, metric)\n",
        "        col_name_avg=f\"{group}-avg\"\n",
        "        for metric in metrics:\n",
        "            results[metric][col_name_avg]=compute_mean(group_examples, metric)\n",
        "\n",
        "    for ui in ui_types:\n",
        "        ui_examples = [ex for ex in list_of_examples if ex.get(\"ui_type\")==ui]\n",
        "        col_name=f\"All-{ui}\"\n",
        "        for metric in metrics:\n",
        "            results[metric][col_name]=compute_mean(ui_examples, metric)\n",
        "    for metric in metrics:\n",
        "        results[metric][\"All-avg\"]=compute_mean(list_of_examples, metric)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating microsoft/GUI-Actor-7B-Qwen2.5-VL ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "540bcba789bc4fcc8084b3739a2c7a79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9865e7e0572746ec9bd980eda0382417",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/248 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 23/1581 [08:27<8:33:56, 19.79s/it] "
          ]
        }
      ],
      "source": [
        "# ================= 6️⃣ Chạy evaluate và lưu kết quả =================\n",
        "resize_to_pixels = RESIZE_TO_PIXELS if RESIZE_TO_PIXELS > 0 else None\n",
        "\n",
        "if os.path.exists(METRIC_PATH):\n",
        "    print(f\"Metrics already exist at {METRIC_PATH}\")\n",
        "else:\n",
        "    if os.path.exists(PRED_PATH):\n",
        "        print(f\"Loading predictions from {PRED_PATH}\")\n",
        "        with open(PRED_PATH, \"r\") as f:\n",
        "            results = json.load(f)\n",
        "    else:\n",
        "        print(f\"Evaluating {MODEL_NAME_OR_PATH} ...\")\n",
        "        results = evaluate(MODEL_NAME_OR_PATH, MODEL_TYPE, DATA_FN, IMAGE_DIR, USE_PLACEHOLDER, TOPK, resize_to_pixels)\n",
        "        with open(PRED_PATH, \"w\") as f:\n",
        "            json.dump(results, f)\n",
        "        print(f\"Saved {len(results)} predictions to {PRED_PATH}\")\n",
        "\n",
        "    metric_info = get_metric(results)\n",
        "    with open(METRIC_PATH, \"w\") as f:\n",
        "        json.dump(metric_info, f, indent=2)\n",
        "    print(f\"Saved metric to {METRIC_PATH}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "yosim",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "052b3cbaaf854241a5a9006b44b4164d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e7d0228b5b24d0ba23a493264adb815": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": "10px 0 0 0",
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "300px"
          }
        },
        "14fdf26fefb4453381b45dd79a53b42f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_668ad79a393f4d21bf601b2354ae3e84",
            "placeholder": "​",
            "style": "IPY_MODEL_7ea64cf5a7064a5fbf870f46616187e6",
            "value": "\n<div class='custom-container'>\n    <style>\n        .custom-container {\n            margin-top: 10px;\n            margin-bottom: 0;\n        }\n        .spaced {\n            margin: 20px 0;\n        }\n    </style>\n    <center>\n        <img src=https://d2vchdhjlcm3i6.cloudfront.net/Company+Logo/Light/Company+logo_light.svg width=175 alt=\"Jina AI\">\n        <div class='spaced'></div>\n        <p>\n            Please open <a href='https://jina-ai.us.auth0.com/authorize?response_mode=form_post&nonce=fe52fd0fa296f2302b33aab321a97623&state=fe52fd0fa296f2302b33aab321a97623&scope=profile+openid+email&redirect_uri=https%3A%2F%2Fapi.hubble.jina.ai%2Fv2%2Foidc%2FidpAuthorized&client_id=7pXAUAtiRqruNd6KJ6U3Zd9uhk5oLqZA&response_type=code&code_challenge=_XDNsaIW8OxdXBPEROkXG-mMtZlynYZuII8TN_l-SUA&code_challenge_method=S256' target='_blank'>this link</a> to continue the login process.\n        </p>\n    </center>\n</div>\n"
          }
        },
        "2e16156e982840e7bcf859bc8bac43c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35202cf8b3b64938b0437e3433ee7d1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5361787acc0948be974a31bd054632b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35202cf8b3b64938b0437e3433ee7d1e",
            "placeholder": "​",
            "style": "IPY_MODEL_2e16156e982840e7bcf859bc8bac43c0",
            "value": "\n<div class='custom-container'>\n    <style>\n        .custom-container {\n            margin-top: 10px;\n            margin-bottom: 0;\n        }\n        .spaced {\n            margin: 20px 0;\n        }\n    </style>\n    <center>\n        <img src=https://d2vchdhjlcm3i6.cloudfront.net/Company+Logo/Light/Company+logo_light.svg width=175 alt='Jina AI'>\n        <div class='spaced'></div>\n        <p>\n            You are logged in to Jina AI!\n        </p>\n        <p>\n            If you want to log in again, run <code>notebook_login(force=True)</code>.\n        </p>\n    </center>\n</div>\n"
          }
        },
        "5640c168070347138c6f12a89f3527f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74d5ed1405e14e7b82fa5c439509b08d",
              "IPY_MODEL_d22020b9cdcb4db5b903ec0510d77bf9",
              "IPY_MODEL_7a2cb071664a4ec985769c655941b3ab",
              "IPY_MODEL_c7ba7e7ad7a1478498bd1b7035cfa262"
            ],
            "layout": "IPY_MODEL_052b3cbaaf854241a5a9006b44b4164d"
          }
        },
        "59221f747d3d472a8eb4c7e8a819bf71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5361787acc0948be974a31bd054632b3"
            ],
            "layout": "IPY_MODEL_052b3cbaaf854241a5a9006b44b4164d"
          }
        },
        "668ad79a393f4d21bf601b2354ae3e84": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74d5ed1405e14e7b82fa5c439509b08d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8429c73d5554f7e9f6578a7c04557af",
            "placeholder": "​",
            "style": "IPY_MODEL_85713ec3389446e6869e69a262023f43",
            "value": "\n<div class='custom-container'>\n    <style>\n        .button1 {\n            color: white;\n            background-color: #009191;\n            border: 1px solid #009191;\n        }\n        .button2 {\n            color: #009191;\n            background-color: white;\n            border: 1px solid #009191;\n        }\n        .link1 {\n            color:#009191;\n            position: relative;\n            top: 22px;\n            right: -120px;\n            z-index: 99;\n        }\n        .custom-container {\n            margin-top: 10px;\n            margin-bottom: -10px;\n        }\n        .spaced {\n            margin: 20px 0;\n        }\n    </style>\n    <center>\n        <img src=https://d2vchdhjlcm3i6.cloudfront.net/Company+Logo/Light/Company+logo_light.svg width=175 alt='Jina AI'>\n        <div class='spaced'></div>\n        <p>\n            Copy a <b>Personal Access Token</b>, paste it below, and press the <b>Token login</b> button.\n            <br>\n            If you don't have a token, press the <b>Browser login</b> button to log in via the browser.\n        </p>\n        <a\n            href='https://hub.jina.ai/user/tokens'\n            target='__blank'\n            class='link1'>\n                Create\n        </a>\n    </center>\n</div>\n"
          }
        },
        "77fd7e85ea4346c5bb3886f2f4c42bca": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_eedc32bcc61a430dadec9bca5a508745",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠦</span> Preparing to run, logs will be ready to pull when `status` is `STARTED`. Current status is `CREATED`\n</pre>\n",
                  "text/plain": "\u001b[32m⠦\u001b[0m Preparing to run, logs will be ready to pull when `status` is `STARTED`. Current status is `CREATED`\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "7a2cb071664a4ec985769c655941b3ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [
              "button1"
            ],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Token login",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_f27061fd69d24cd6a77a694aec8906f2",
            "style": "IPY_MODEL_b2492a07ae4649f29c0d0a8c23c4047e",
            "tooltip": ""
          }
        },
        "7ea64cf5a7064a5fbf870f46616187e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85713ec3389446e6869e69a262023f43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a56509f7c23d495fa81dc07b5ed38419": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14fdf26fefb4453381b45dd79a53b42f"
            ],
            "layout": "IPY_MODEL_052b3cbaaf854241a5a9006b44b4164d"
          }
        },
        "b2492a07ae4649f29c0d0a8c23c4047e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c7ba7e7ad7a1478498bd1b7035cfa262": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [
              "button2"
            ],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Browser login",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_0e7d0228b5b24d0ba23a493264adb815",
            "style": "IPY_MODEL_f75a1fb831f64ca59dfeb7ed8e411483",
            "tooltip": ""
          }
        },
        "c8429c73d5554f7e9f6578a7c04557af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdd962679dbb49e3873289ef6bd13633": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d22020b9cdcb4db5b903ec0510d77bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": true,
            "layout": "IPY_MODEL_cdd962679dbb49e3873289ef6bd13633",
            "placeholder": "Personal Access Token (PAT)",
            "style": "IPY_MODEL_f4b7f904482d4e5d9ab526fe7897f256",
            "value": ""
          }
        },
        "da795a9e9c8c47c489b520483c95713a": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f33b28cfdc71491baa49f843b1f51389",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠋</span> <span style=\"font-weight: bold\">Downloading</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008000; text-decoration-color: #008000\">950009856/953017547</span> • <span style=\"color: #800000; text-decoration-color: #800000\">15933119 QPS</span> • <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> • <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">953.0 MB</span>\n</pre>\n",
                  "text/plain": "\u001b[32m⠋\u001b[0m \u001b[1mDownloading\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m950009856/953017547\u001b[0m • \u001b[31m15933119 QPS\u001b[0m • \u001b[36m0:00:01\u001b[0m • \u001b[1;34m953.0 MB\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "eedc32bcc61a430dadec9bca5a508745": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f27061fd69d24cd6a77a694aec8906f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "300px"
          }
        },
        "f33b28cfdc71491baa49f843b1f51389": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4b7f904482d4e5d9ab526fe7897f256": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f75a1fb831f64ca59dfeb7ed8e411483": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
